# -*- coding: utf-8 -*-
"""Copy of Lista9_5_Student.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bLk9pIVzHaQi5Quyu0GWCcFeur0qD2BW
"""

import numpy as np
import matplotlib.pyplot as plt
import time
import os
import copy

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from datetime import datetime
from torchvision import datasets, models, transforms
from torchsummary import summary

"""## Treinamento a partir de uma base propria de imagens - Intel Image Classification

<td>
<img src="https://drive.google.com/uc?id=1kr__8xXIyPxIdoBaoZVKEdg0y8i8-jo7" alt="LeNet"  width="500"/> </td>





<p>
Context: This is image data of Natural Scenes around the world.
</p>

<p>
Content: This Data contains around 25k images of size 150x150 distributed under 6 categories.
  <ul>
    <li>buildings</li>
    <li>forest</li>
    <li>glacier</li>
    <li>mountain</li>
    <li>sea</li>
    <li>street</li>
  </ul>
</p>

<p>
Fonte: <a href="https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download-directory>Intel Image Classification</a>
</p>
 <a href="https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download-directory>Intel Image Classification</a>

### Usar Gdown para realizar a descarga de um arquivo desde o drive
"""

# instalação da biblioteca gdown
!pip install gdown

# importando a biblioteca
import gdown

"""
<img src="https://drive.google.com/uc?id=1VgEEHaw-5798fIY03iWIYOIYK4-kkiNW" alt="Drawing"  width="900"/>

```
gdown --id <put-the-ID>
```"""

# Realizando a descarga usando o id do gdrive do arquivo
!gdown --id 16oJl8kDIBMvKdeAkCqXrmZCQMi9lb8ft

# biblioteca para decomprimir o arquivo zip
import zipfile
import os

# funçao que realiza a descarga
def unzip_dataset(filename, directory):
  """
  filename: nome do arquivo, incluir o endereço onde está localizado o arquivo
  directory: pasta onde será decomprimido o arquivo
  """
  zip_ref = zipfile.ZipFile(filename, 'r')
  zip_ref.extractall(directory)
  zip_ref.close()

# Top level data directory. Here we assume the format of the directory conforms
#   to the ImageFolder structure
data_dir = '/unzip/'
unzip_dataset(filename='/content/dataset.zip', directory=data_dir )

# Commented out IPython magic to ensure Python compatibility.
# delete files and directories
#%rm -r /tmp/*

# %ls /unzip/test

"""### Preparar o dataset"""

input_size = 224
# Aumento e normalização de dados para treinamento
# Apenas normalização para validação
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(input_size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(input_size),
        transforms.CenterCrop(input_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

print("Inializando Datasets e Dataloaders...")


# Criar conjuntos de dados de treinamento e validação
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}

# conjunto de treino é divido em treino (80%) e validação (20%)
train_dataset_split, val_dataset_split = torch.utils.data.random_split(image_datasets['train'], [0.8, 0.2])

# é adicionado no dicionario image_datasets o conjunto de validação e o conjunto de treino é atualizado
image_datasets['val'] = val_dataset_split
image_datasets['train'] = train_dataset_split

"""### Definição de alguns parâmetros"""

# Number of classes in the dataset
num_classes = 6

# Batch size for training (change depending on how much memory you have)
batch_size = 8

# Number of epochs to train for
num_epochs = 15

# Flag for feature extracting. When False, we finetune the whole model,
#   when True we only update the reshaped layer params
feature_extract = True

# Criar dataloaders de treinamento e validação
dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'val','test']}

# Detectar a disponibilidade de uma GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""### Função que congela ou descongela as camadas da rede, por *default* são carregadas descongeladas, ou seja, permite que todos os pesos da rede sejam atualizados durante o treino."""

def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

"""#### Função que realiza o treino"""

def fit(model, criterion, optimizer, train_loader, test_loader, epochs):
    train_losses = np.zeros( epochs )
    test_losses = np.zeros( epochs )

    for it in range(epochs):
        model.train()
        t0 = datetime.now()
        train_loss = []

        for inputs, targets in train_loader:
            # mover os dados para o GPU
            inputs, targets = inputs.to(device), targets.to(device)

            # zerar os gradientes
            optimizer.zero_grad()

            # forward pass
            outputs = model(inputs)

            loss = criterion(outputs, targets)

            # backward pass e optimiza
            loss.backward()
            optimizer.step()

            train_loss.append( loss.item() )

        train_loss = np.mean(train_loss)

        test_loss = []
        for inputs, targets in test_loader:
            model.eval()
            # mover os dados para o GPU
            inputs, targets = inputs.to(device), targets.to(device)

            # forward pass
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            test_loss.append(loss.item())
        test_loss = np.mean(test_loss)

        # salvar losses
        train_losses[it] = train_loss
        test_losses[it] = test_loss

        dt = datetime.now() - t0

        print(f'Epoch {it+1}/{epochs} Train Loss: {train_loss:.4f} Test Loss: {test_loss} Duracao: {dt}')

    return train_losses, test_losses

"""### Função que grafica as funções de perda(custo) do treino e da validação"""

def plot_losses(train_losses, test_losses):
    plt.plot(train_losses, label = 'train loss')
    plt.plot(test_losses, label = 'test loss')
    plt.legend()
    plt.show()

"""### Função que realiza a predição com a rede já treinada"""

def predict_module(model, test_loader):
    model.eval()
    predicted = np.empty((0), int)
    true_labels = np.empty((0), int)
    for inputs, targets in test_loader:
        # mover os dados para o GPU
        inputs, targets = inputs.to(device), targets.to(device)

        outputs = model(inputs)

        # obter a predição
        _, predictions = torch.max(outputs, 1)

        predicted = np.append(predicted, predictions.cpu())
        true_labels = np.append(true_labels, targets.cpu())

    return predicted, true_labels

"""### Função grafica a matriz de confusão como um heatmap"""

from sklearn import metrics

def matriz_confusao(true_labels, predicted, display_labels=[0,1,2]):
    matriz = metrics.confusion_matrix(true_labels, predicted)
    tot = np.sum(matriz, axis=1, keepdims=True)
    np.set_printoptions(precision=2)
    m_porc = matriz/tot
    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=m_porc, display_labels=display_labels)
    disp.plot(values_format='.2f')
    #fig, ax = plt.subplots(figsize=(10,10))
    #disp.plot(ax=ax, xticks_rotation='vertical')
    plt.show()

"""# Questão 1 - Treinar uma rede Transformer"""

model_tr = models.vit_b_32(weights='ViT_B_32_Weights.IMAGENET1K_V1')
print(model_tr)

# congela os pesos de todas as camadas da rede
set_parameter_requires_grad(model_tr, True)
# encontra o número de neurônios na última camada (camada fc)
num_ftrs = model_tr.heads[0].in_features
num_classes = model_tr.heads[0].out_features

print("Ultima camada")
print("Neuronios:", num_ftrs)
print("Classes:", num_classes)

# troca a camada anterior por uma nova camada full connected. A saída da camada é igual ao número de classes de nossa base de imagens
model_tr.heads.head = nn.Linear(num_ftrs, num_classes)

print(model_tr)

model_tr = model_tr.to(device)
optimizer_model_tr = optim.SGD(model_tr.parameters(), lr=0.001, momentum=0.9)
criterion_model_tr  = nn.CrossEntropyLoss()

train_losses, test_losses = fit(model_tr , criterion_model_tr , optimizer_model_tr , dataloaders_dict['train'], dataloaders_dict['val'], epochs = 10)
plot_losses(train_losses, test_losses)

# Avaliando com o conjunto de teste
predicted, true_labels = predict_module(model_tr, dataloaders_dict['test'])
matriz_confusao(true_labels, predicted, ['Buildings', 'Forest', 'Glacier', 'Mountain', 'Sea', 'Street'])
